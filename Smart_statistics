# ğŸ“¦ Ù…ÙƒØªØ¨Ø© Ø¯ÙˆØ§Ù„ Ø°ÙƒÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import (
    ttest_ind, ttest_1samp, ttest_rel,
    f_oneway, chi2_contingency, shapiro, probplot
)
from sklearn.linear_model import LinearRegression, LogisticRegression

# âœ… 1. Independent T-Test
def t_test_independent_from_df(df, col, group_col, group1_val, group2_val, alpha=0.05):
    g1 = df[df[group_col] == group1_val][col].dropna()
    g2 = df[df[group_col] == group2_val][col].dropna()
    t_stat, p_val = ttest_ind(g1, g2)
    print(f"Independent T-Test between {group1_val} and {group2_val} on '{col}':")
    print(f"T = {t_stat:.4f}, P = {p_val:.4f}")
    print("âœ… Significant difference" if p_val < alpha else "âŒ No significant difference")

# âœ… 2. One Sample T-Test
def t_test_one_sample_from_df(df, col, popmean, alpha=0.05):
    t_stat, p_val = ttest_1samp(df[col].dropna(), popmean)
    print(f"One Sample T-Test on '{col}' vs population mean {popmean}:")
    print(f"T = {t_stat:.4f}, P = {p_val:.4f}")
    print("âœ… Mean is significantly different" if p_val < alpha else "âŒ No significant difference")

# âœ… 3. Paired T-Test
def t_test_paired_from_df(df, col_before, col_after, alpha=0.05):
    before = df[col_before].dropna()
    after = df[col_after].dropna()
    t_stat, p_val = ttest_rel(before, after)
    print(f"Paired T-Test between '{col_before}' and '{col_after}':")
    print(f"T = {t_stat:.4f}, P = {p_val:.4f}")
    print("âœ… Significant change" if p_val < alpha else "âŒ No significant change")

# âœ… 4. ANOVA Test
def run_anova_from_df(df, col, group_col, alpha=0.05):
    groups = [g[col].dropna() for _, g in df.groupby(group_col)]
    stat, p_val = f_oneway(*groups)
    print(f"ANOVA on '{col}' by '{group_col}':")
    print(f"F = {stat:.4f}, P = {p_val:.4f}")
    print("âœ… Significant difference between groups" if p_val < alpha else "âŒ No significant difference")

# âœ… 5. Chi-Square Test
def chi_square_from_df(df, col1, col2, alpha=0.05):
    table = pd.crosstab(df[col1], df[col2])
    chi2, p, _, _ = chi2_contingency(table)
    print(f"Chi-Square Test between '{col1}' and '{col2}':")
    print(f"Chi2 = {chi2:.4f}, P = {p:.4f}")
    print("âœ… Variables are dependent" if p < alpha else "âŒ Variables are independent")

# âœ… 6. Correlation Matrix
def show_correlation_matrix(df):
    corr = df.corr()
    print("\nCorrelation Matrix:")
    print(corr.round(2))
    plt.figure(figsize=(8, 6))
    sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
    plt.title("Correlation Matrix")
    plt.tight_layout()
    plt.show()

# âœ… 7. Normality Check (Histogram + Q-Q Plot + Shapiro)
def check_normality(data, alpha=0.05):
    plt.figure(figsize=(6, 4))
    sns.histplot(data.dropna(), kde=True, color='skyblue', bins=30)
    plt.title("Histogram + KDE")
    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(6, 4))
    probplot(data.dropna(), dist="norm", plot=plt)
    plt.title("Q-Q Plot")
    plt.tight_layout()
    plt.show()

    stat, p_val = shapiro(data.dropna())
    print(f"Shapiro-Wilk Test: P = {p_val:.4f}")
    print("âœ… Normally distributed" if p_val > alpha else "âŒ Not normally distributed")

# âœ… 8. Linear Regression Report
def run_linear_regression_from_df(df, feature_cols, target_col):
    X = df[feature_cols].dropna()
    y = df[target_col].dropna()
    model = LinearRegression()
    model.fit(X, y)
    print("\nLinear Regression Results:")
    print("Intercept:", model.intercept_)
    print("Coefficients:")
    for name, coef in zip(feature_cols, model.coef_):
        print(f"- {name}: {coef:.4f}")
    return model

# âœ… 9. Logistic Regression Report
def run_logistic_regression_from_df(df, feature_cols, target_col):
    X = df[feature_cols].dropna()
    y = df[target_col].dropna()
    model = LogisticRegression(max_iter=1000)
    model.fit(X, y)
    print("\nLogistic Regression Model Trained.")
    print("Intercept:", model.intercept_)
    print("Coefficients:")
    for name, coef in zip(feature_cols, model.coef_[0]):
        print(f"- {name}: {coef:.4f}")
    return model




def get_top_correlated_features(df, target_col, top_n=5, plot=False):
    """
    ØªØ¹Ø±Ø¶ Ø£ÙƒØ«Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ Ø¨Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ù‡Ø¯Ù (target_col) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Pearson correlation.
    
    Parameters:
    - df: DataFrame ÙƒØ§Ù…Ù„
    - target_col: Ø§Ø³Ù… Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‡Ø¯Ù (Ù…Ø«Ø§Ù„: 'Diabetes')
    - top_n: Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø¹Ù„Ù‰ Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ (Ø§ÙØªØ±Ø§Ø¶ÙŠ = 5)
    - plot: Ø¥Ø°Ø§ TrueØŒ ÙŠØ±Ø³Ù… Ø¨Ø§Ø± Ø´Ø§Ø±Øª Ù„Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø¹Ù„Ù‰
    
    Returns:
    - DataFrame ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ù…ÙŠØ²Ø§Øª + Ù‚ÙŠÙ…Ø© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· (Ù…Ø±ØªØ¨Ø© ØªÙ†Ø§Ø²Ù„ÙŠÙ‹Ø§)
    """
    import pandas as pd
    import matplotlib.pyplot as plt

    # Ø§Ù„ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ù‡Ø¯Ù Ù…ÙˆØ¬ÙˆØ¯
    if target_col not in df.columns:
        raise ValueError(f"'{target_col}' not found in dataframe.")

    # Ø­Ø³Ø§Ø¨ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·
    corr = df.corr()[target_col].drop(labels=[target_col])
    corr_df = corr.abs().sort_values(ascending=False).head(top_n)

    print(f"\nğŸ” Top {top_n} features most correlated with '{target_col}':\n")
    print(corr_df)

    if plot:
        plt.figure(figsize=(8, 4))
        corr_df.plot(kind='barh', color='teal')
        plt.gca().invert_yaxis()
        plt.title(f"Top {top_n} Features Correlated with '{target_col}'")
        plt.xlabel("Absolute Correlation")
        plt.tight_layout()
        plt.show()

    return corr_df
